{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 2.1 Hadoop MapReduce with Python\n",
    "There are two prominent *Python* APIs for interfacing *Hadoop MapReduce* clusters:\n",
    "\n",
    "## *Snakebite* for *HDFS* access\n",
    "The [Snakebite Lib](https://github.com/spotify/snakebite) allows easy access to *HDFS* file systems:  \n",
    "```\n",
    ">>> from snakebite.client import Client\n",
    ">>> client = Client(\"localhost\", 8020, use_trash=False)\n",
    ">>> for x in client.ls(['/']):\n",
    "...     print x\n",
    "```\n",
    "\n",
    "See [documentation](https://snakebite.readthedocs.io/en/latest/) for details.\n",
    "\n",
    "\n",
    "## *MRJOB* for *MapReduce* job execution\n",
    "The ``mrjob`` lib -> [see docu](https://mrjob.readthedocs.io/en/latest/index.html) is a power full *MapReduce* client for *Python*. Some of the key features are:\n",
    "\n",
    "* local emulation (single and multi-core) a *Hadoop* cluster for development and debugging\n",
    "* simple access, authentication and file transfer to *Hadoop* clusters\n",
    "* powerful API for common cloud services, such as AWS or Azure   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/andrea/anaconda3/envs/tf-gpu\n",
      "\n",
      "  added / updated specs:\n",
      "    - boto3\n",
      "    - mrjob\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    boto3-1.17.86              |     pyhd8ed1ab_0          70 KB  conda-forge\n",
      "    botocore-1.20.87           |     pyhd8ed1ab_0         4.7 MB  conda-forge\n",
      "    ca-certificates-2021.5.30  |       ha878542_0         136 KB  conda-forge\n",
      "    certifi-2021.5.30          |   py39hf3d152e_0         141 KB  conda-forge\n",
      "    google-api-core-1.26.3     |     pyhd8ed1ab_0          59 KB  conda-forge\n",
      "    google-api-python-client-2.7.0|     pyhd8ed1ab_0         3.9 MB  conda-forge\n",
      "    google-auth-httplib2-0.1.0 |     pyhd8ed1ab_0          13 KB  conda-forge\n",
      "    googleapis-common-protos-1.53.0|   py39hf3d152e_0         127 KB  conda-forge\n",
      "    httplib2-0.19.1            |     pyhd8ed1ab_0          94 KB  conda-forge\n",
      "    jmespath-0.10.0            |     pyh9f0ad1d_0          21 KB  conda-forge\n",
      "    mrjob-0.7.4                |   py39hf3d152e_0         537 KB  conda-forge\n",
      "    rapidjson-1.1.0            |    he1b5a44_1002         104 KB  conda-forge\n",
      "    s3transfer-0.4.2           |     pyhd8ed1ab_0          55 KB  conda-forge\n",
      "    simplejson-3.17.2          |   py39h07f9747_1         102 KB  conda-forge\n",
      "    ujson-1.35                 |py39h41458e0_1003          29 KB  conda-forge\n",
      "    uritemplate-3.0.1          |             py_0          16 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        10.1 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  boto3              conda-forge/noarch::boto3-1.17.86-pyhd8ed1ab_0\n",
      "  botocore           conda-forge/noarch::botocore-1.20.87-pyhd8ed1ab_0\n",
      "  google-api-core    conda-forge/noarch::google-api-core-1.26.3-pyhd8ed1ab_0\n",
      "  google-api-python~ conda-forge/noarch::google-api-python-client-2.7.0-pyhd8ed1ab_0\n",
      "  google-auth-httpl~ conda-forge/noarch::google-auth-httplib2-0.1.0-pyhd8ed1ab_0\n",
      "  googleapis-common~ conda-forge/linux-64::googleapis-common-protos-1.53.0-py39hf3d152e_0\n",
      "  httplib2           conda-forge/noarch::httplib2-0.19.1-pyhd8ed1ab_0\n",
      "  jmespath           conda-forge/noarch::jmespath-0.10.0-pyh9f0ad1d_0\n",
      "  mrjob              conda-forge/linux-64::mrjob-0.7.4-py39hf3d152e_0\n",
      "  rapidjson          conda-forge/linux-64::rapidjson-1.1.0-he1b5a44_1002\n",
      "  s3transfer         conda-forge/noarch::s3transfer-0.4.2-pyhd8ed1ab_0\n",
      "  simplejson         conda-forge/linux-64::simplejson-3.17.2-py39h07f9747_1\n",
      "  ujson              conda-forge/linux-64::ujson-1.35-py39h41458e0_1003\n",
      "  uritemplate        conda-forge/noarch::uritemplate-3.0.1-py_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates                      2020.12.5-ha878542_0 --> 2021.5.30-ha878542_0\n",
      "  certifi                          2020.12.5-py39hf3d152e_1 --> 2021.5.30-py39hf3d152e_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "google-api-python-cl | 3.9 MB    | ##################################### | 100% \n",
      "boto3-1.17.86        | 70 KB     | ##################################### | 100% \n",
      "google-auth-httplib2 | 13 KB     | ##################################### | 100% \n",
      "googleapis-common-pr | 127 KB    | ##################################### | 100% \n",
      "httplib2-0.19.1      | 94 KB     | ##################################### | 100% \n",
      "uritemplate-3.0.1    | 16 KB     | ##################################### | 100% \n",
      "simplejson-3.17.2    | 102 KB    | ##################################### | 100% \n",
      "google-api-core-1.26 | 59 KB     | ##################################### | 100% \n",
      "ca-certificates-2021 | 136 KB    | ##################################### | 100% \n",
      "rapidjson-1.1.0      | 104 KB    | ##################################### | 100% \n",
      "ujson-1.35           | 29 KB     | ##################################### | 100% \n",
      "botocore-1.20.87     | 4.7 MB    | ##################################### | 100% \n",
      "s3transfer-0.4.2     | 55 KB     | ##################################### | 100% \n",
      "certifi-2021.5.30    | 141 KB    | ##################################### | 100% \n",
      "jmespath-0.10.0      | 21 KB     | ##################################### | 100% \n",
      "mrjob-0.7.4          | 537 KB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "#install mrjob lib and boto3 for AWS S3 access\n",
    "!conda install -c conda-forge -y mrjob boto3\n",
    "\n",
    "#or !pip install mrjob boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A *MRJOB* Example: WordCount (again)\n",
    "Since *Hadoop* works only on file in- and outputs, we do not have usual function based API. We need to pass our code (implementation of *Map* and *Reduce*) as executable *Python* scripts:\n",
    "\n",
    "* use *Jupyter's* ``%%file`` magic command to write the cell to file\n",
    "* create a executable script with ``__main__`` method\n",
    "* inherit from the ``MRJob`` class\n",
    "* implement ``mapper()`` and ``reducer()`` methods\n",
    "* call ``run()`` at start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wordcount.py\n"
     ]
    }
   ],
   "source": [
    "%%file wordcount.py \n",
    "#this will save this cell as file\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRWordCount(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "        for word in line.split():\n",
    "            yield(word, 1)\n",
    " \n",
    "    def reducer(self, word, counts):\n",
    "        yield(word, sum(counts))\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRWordCount.run()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### execute script from cmd\n",
    "* ``-r local`` causes local multi-core emulation a *Hadoop* cluster.\n",
    "* Input files are cmd arguments\n",
    "* define ouput-file (see docs) or use streams: `` > out.txt``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for local runner\n",
      "Creating temp directory /tmp/wordcount.andrea.20210604.123534.071240\n",
      "Running step 1 of 1...\n",
      "job output is in /tmp/wordcount.andrea.20210604.123534.071240/output\n",
      "Streaming final output from /tmp/wordcount.andrea.20210604.123534.071240/output...\n",
      "Removing temp directory /tmp/wordcount.andrea.20210604.123534.071240...\n"
     ]
    }
   ],
   "source": [
    "! python wordcount.py -r local *.rst > out.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -> results in **out.txt** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution on AWS EMR\n",
    "AWS EMR is a clound formation service which allows you to create *Hadoop*, *Spark* and other data analytics clusters with a few clicks.\n",
    "\n",
    "**NOTE**: we are not endorsing AWS specifically, other cloud service providers have similar offers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to existing cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mrjob_cluster.conf\n"
     ]
    }
   ],
   "source": [
    "%%file mrjob_cluster.conf\n",
    "runners:\n",
    "  emr:\n",
    "    #aws_access_key_id: YOUR_KEY_ID\n",
    "    aws_access_key_id: AKIA4KIF2TSEWXWXQI76 \n",
    "    #aws_secret_access_key: YOUR_KEY_SECRET\n",
    "    aws_secret_access_key: pGKnuZYi3HGCK5yAnCWaOMpofDNl7F0CmZDypHfx \n",
    "    region: eu-west-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the **ID** of the cluster we want to connect to - here pre-set to our Cluster today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python wordcount.py -r emr --cluster-id=j-L1BO0NYZIYY0 text1.rst text2.rst -c mrjob_cluster.conf  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using s3://mrjob-42e7145df80ebe94/tmp/ as our temp dir on S3\n",
      "Creating temp directory /tmp/topannual.andrea.20210604.142304.971084\n",
      "uploading working dir files to s3://mrjob-42e7145df80ebe94/tmp/topannual.andrea.20210604.142304.971084/files/wd...\n",
      "Copying other local files to s3://mrjob-42e7145df80ebe94/tmp/topannual.andrea.20210604.142304.971084/files/\n",
      "Adding our job to existing cluster j-L1BO0NYZIYY0\n",
      "  master node is ec2-52-212-16-94.eu-west-1.compute.amazonaws.com\n",
      "Waiting for Step 1 of 1 (s-1PLPFAK10P0MF) to complete...\n",
      "  PENDING (cluster is RUNNING: Running step)\n",
      "  RUNNING for 0:00:43\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Waiting 10 minutes for logs to transfer to S3... (ctrl-c to skip)\n",
      "\n",
      "To fetch logs immediately next time, set up SSH. See:\n",
      "https://pythonhosted.org/mrjob/guides/emr-quickstart.html#configuring-ssh-credentials\n",
      "\n",
      "Looking for step log in s3://aws-logs-846657657993-eu-west-1/elasticmapreduce/j-L1BO0NYZIYY0/steps/s-1PLPFAK10P0MF...\n",
      "  Parsing step log: s3://aws-logs-846657657993-eu-west-1/elasticmapreduce/j-L1BO0NYZIYY0/steps/s-1PLPFAK10P0MF/syslog.gz\n",
      "Counters: 55\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1783405\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=210\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=213523\n",
      "\t\tFILE: Number of bytes written=2895274\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1384\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=8\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tS3: Number of bytes read=1783405\n",
      "\t\tS3: Number of bytes written=210\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=8\n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=8\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=245889024\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=107814912\n",
      "\t\tTotal time spent by all map tasks (ms)=80042\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7684032\n",
      "\t\tTotal time spent by all reduce tasks (ms)=17548\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3369216\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=80042\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=17548\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=20460\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=3669\n",
      "\t\tInput split bytes=1384\n",
      "\t\tMap input records=18982\n",
      "\t\tMap output bytes=591570\n",
      "\t\tMap output materialized bytes=216884\n",
      "\t\tMap output records=18982\n",
      "\t\tMerged Map outputs=24\n",
      "\t\tPhysical memory (bytes) snapshot=6633902080\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=18982\n",
      "\t\tReduce output records=1\n",
      "\t\tReduce shuffle bytes=216884\n",
      "\t\tShuffled Maps =24\n",
      "\t\tSpilled Records=37964\n",
      "\t\tTotal committed heap usage (bytes)=6204424192\n",
      "\t\tVirtual memory (bytes) snapshot=59347861504\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in s3://mrjob-42e7145df80ebe94/tmp/topannual.andrea.20210604.142304.971084/output/\n",
      "Streaming final output from s3://mrjob-42e7145df80ebe94/tmp/topannual.andrea.20210604.142304.971084/output/...\n",
      "[238772.0, 200000.0, 193800.0, 190000.0, 187200.0, 172000.0, 165000.0, 163365.0, 163200.0, 163200.0]\t[238772.04, 193653.69, 188328.5, 185741.81, 176141.33, 173876.84, 166442.42, 165892.21, 165270.01, 165108.5]\n",
      "Removing s3 temp directory s3://mrjob-42e7145df80ebe94/tmp/topannual.andrea.20210604.142304.971084/...\n",
      "Removing temp directory /tmp/topannual.andrea.20210604.142304.971084...\n"
     ]
    }
   ],
   "source": [
    "! python topannual.py -r emr --cluster-id=j-L1BO0NYZIYY0 Baltimore_City_Employee_Salaries_FY2014.csv -c mrjob_cluster.conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Use  *mrjob*  to  compute  employee  **top  annual  salaries** and  **gross pay** in the *CSV* table ``Baltimore_City_employee_Salaries_FY2014.csv``.\n",
    "\n",
    "* use  ``import csv`` to read the data -> [API docs](https://docs.python.org/3/library/csv.html)\n",
    "* use ``yield`` to return *producers* from *map* and *reduce* functions\n",
    "* return top entries in both categories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing topannual.py\n"
     ]
    }
   ],
   "source": [
    "%%file topannual.py \n",
    "#this will save this cell as file\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "import csv\n",
    "\n",
    "class MRWordCount(MRJob):\n",
    "\n",
    "    def mapper(self, _, line): \n",
    "        reader = line.split(',') \n",
    "        salary = reader[-2] \n",
    "        grosspay=reader[-1] \n",
    "        \n",
    "        try: \n",
    "            float(salary) \n",
    "            float(grosspay) \n",
    "        except: \n",
    "            salary = 0 \n",
    "            grosspay = 0 \n",
    "        yield(\"topentries\", (float(salary), float(grosspay))) \n",
    "        \n",
    "    def reducer(self, key, values): \n",
    "        salary, grosspay = zip(*values) \n",
    "        salary, grosspay = list(salary), list(grosspay) \n",
    "        salary.sort(reverse=True) \n",
    "        grosspay.sort(reverse=True) \n",
    "        yield(salary[:10], grosspay[:10]) \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for local runner\n",
      "Creating temp directory /tmp/topannual.andrea.20210604.141148.081371\n",
      "Running step 1 of 1...\n",
      "job output is in /tmp/topannual.andrea.20210604.141148.081371/output\n",
      "Streaming final output from /tmp/topannual.andrea.20210604.141148.081371/output...\n",
      "Removing temp directory /tmp/topannual.andrea.20210604.141148.081371...\n"
     ]
    }
   ],
   "source": [
    "! python topannual.py -r local *.csv > topannual.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
